# Semantic Claw Router — Example Configuration
# Routes between local Qwen3-Coder-Next (vLLM on OpenShift) and Google Gemini
#
# Before running, set environment variables:
#
#   # Option 1: Source from OpenShift cluster secret
#   export GEMINI_API_KEY=$(oc get secret gemini-api-key -n aiops-harness \
#       -o jsonpath='{.data.GEMINI_API_KEY}' | base64 -d)
#   export VLLM_ENDPOINT=$(oc get route qwen3-coder-next -n llm-serving \
#       -o jsonpath='https://{.spec.host}')
#
#   # Option 2: Set directly
#   export GEMINI_API_KEY="your-google-ai-studio-key"
#   export VLLM_ENDPOINT="https://your-vllm-host.example.com"

host: "0.0.0.0"
port: 8080
request_timeout: 120.0

# ── Model Backends ───────────────────────────────────────────────────
models:
  - name: "qwen3-coder-next"
    provider: "vllm"
    endpoint: "${VLLM_ENDPOINT}"
    context_window: 32768
    cost_per_million_input: 0.0    # Self-hosted, no per-token cost
    cost_per_million_output: 0.0
    supports_tools: true
    supports_streaming: true

  - name: "gemini-2.5-flash"
    provider: "gemini"
    endpoint: "https://generativelanguage.googleapis.com/v1beta"
    api_key: "${GEMINI_API_KEY}"
    context_window: 1048576
    cost_per_million_input: 0.15
    cost_per_million_output: 0.60
    supports_tools: true
    supports_streaming: true

# ── Tier → Model Mapping ─────────────────────────────────────────────
# SIMPLE/MEDIUM: Use free local Qwen3 (cost optimization)
# COMPLEX/REASONING: Use Gemini 2.5 Flash (quality optimization)
default_tier_models:
  SIMPLE: "qwen3-coder-next"
  MEDIUM: "qwen3-coder-next"
  COMPLEX: "gemini-2.5-flash"
  REASONING: "gemini-2.5-flash"

# ── Fast-Path Classifier ─────────────────────────────────────────────
fast_path:
  enabled: true
  confidence_threshold: 0.7
  weights:
    token_count: 0.08
    code_presence: 0.15
    reasoning_markers: 0.18
    technical_terms: 0.10
    creative_markers: 0.05
    simple_indicators: 0.02
    multi_step_patterns: 0.12
    question_complexity: 0.05
    imperative_verbs: 0.03
    constraint_indicators: 0.04
    output_format: 0.03
    reference_complexity: 0.02
    negation_complexity: 0.01
    domain_specificity: 0.02
    agentic_task: 0.04
  tier_boundaries:
    simple: 0.0
    medium: 0.3
    complex: 0.5

# ── Semantic Classifier (Fallback) ──────────────────────────────────
# When the fast-path is ambiguous, use sentence embeddings for
# meaning-based classification.  Requires: pip install semantic-claw-router[semantic]
# If sentence-transformers is not installed, falls back to heuristic scoring.
semantic_classifier:
  enabled: true
  model_name: "all-MiniLM-L6-v2"
  top_k: 3
  # anchors: (uses sensible defaults — override per tier if needed)

# ── Request Deduplication ─────────────────────────────────────────────
dedup:
  enabled: true
  window_seconds: 30
  max_entries: 10000

# ── Session Pinning ───────────────────────────────────────────────────
session:
  enabled: true
  ttl_seconds: 3600
  max_sessions: 10000

# ── Context Compression ──────────────────────────────────────────────
compression:
  enabled: true
  threshold_bytes: 184320
  strategies:
    - whitespace
    - dedup
    - json_compact

# ── Graceful Degradation ─────────────────────────────────────────────
degradation:
  enabled: true
  fallback_model: "qwen3-coder-next"
  triggers:
    - provider_error
    - rate_limit
    - timeout

# ── Observability ─────────────────────────────────────────────────────
observability:
  log_level: "INFO"
  log_format: "text"
  metrics_enabled: true
